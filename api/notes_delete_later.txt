how it currently works:
- at each time step, do the following
=====================
- reads and evaluates the frame with opencv and deepface
- uses pydub and AudioSegment to read the last (timeinterval=10) seconds of the total audio
- exports that audio clip into the audio file folder
- transcribes that clip of audio using speech_recognition library
- gets the sentiment vector (with NLP) of that short clip of text
- compares the vectors, writes the row's string, prints it to the csv.
=====================

how the improved one will work:
- obtains the total audio from the video
- transcribe the entire thing, result is a list of JSON dicts where each one is a word
- print out rows of nulls to the csv for all the ignored time clips (t < 10s, so t-10 < 0)
- for the first clip of time (0 to 10 seconds), get all the words by doing
the following: have the first word be the very first word in the audio,
have the last word be the LAST ONE that BEGINS on or before t=10s,
and get all the words between those, inclusive.
- do the rest of the computation and print out a row for that iteration we just did
- at each time step after, do the following:
---------------
- the last word is the very last one that begins on or before i
- the first word is the very first one that ends on or after {i - time_interval}
- have an array that stores all the words between, inclusive
- can just use sliding window logic for this, no need to be so inefficient
- convert to string, run sentiment analysis
- compare vectors, create string, write to csv, etc.
---------------